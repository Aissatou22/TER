{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeLinksJson(article, append=False):\n",
    "    file = 'dataBase/articles.json'\n",
    "    if append:\n",
    "        with open(file, 'a+') as jsonOut:\n",
    "            json.dump(article, jsonOut, indent=2, ensure_ascii=False)\n",
    "    else:\n",
    "        with open(file, 'w+') as jsonOut:\n",
    "            json.dump(article, jsonOut, indent=2, ensure_ascii=False)\n",
    "            # ensure_ascii=False : pour bien afficher les caractéres accentueux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLinksJson():\n",
    "    file = 'dataBase/articles.json'\n",
    "    if os.path.isfile(file):\n",
    "        with open(file) as jsonIn:\n",
    "            data = json.load(jsonIn)\n",
    "            return data\n",
    "    else:\n",
    "        print(\"I can't find \", file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common(a, b):\n",
    "    c = [value for value in a if value in b]\n",
    "    return c\n",
    "\n",
    "def common2(a, b):\n",
    "    return set(a).intersection(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def articleLinkIdGenerator(online=False):\n",
    "    # on a utilisé le online=false car si on est pas en ligne on pourra\n",
    "    # utiliser la dataBase pour afficher les resultats voulue\n",
    "    url = 'https://pagellapolitica.it/'\n",
    "    uriPage = 'dichiarazioni/verificato?page='\n",
    "    jsonFile = 'dataBase/articles.json'\n",
    "    articles = {}\n",
    "    if online:\n",
    "        for i in range(sys.maxsize):\n",
    "            pageNumber = i\n",
    "            pageNumber = str(pageNumber)\n",
    "            pageAddress = url + uriPage + pageNumber\n",
    "\n",
    "            resp = requests.get(pageAddress)\n",
    "            if resp:\n",
    "                source = requests.get(pageAddress).text\n",
    "                soup = bs.BeautifulSoup(source, \"lxml\")\n",
    "                tables = soup.findAll('div', {'class': 'col-lg-3 mb-7'})\n",
    "                for table in tables:\n",
    "                    uriArticleID = table.find(\n",
    "                        'a', {'class': 'statement-link'})['href']\n",
    "                    idPattern = re.search(r'/([0-9]+)/', uriArticleID)\n",
    "                    idNumber = idPattern.group(1)\n",
    "                    articles[idNumber] = {\n",
    "                        'url': url+uriArticleID,\n",
    "                    }\n",
    "                if idNumber == str(215):\n",
    "                    break\n",
    "            if idNumber == str(215):\n",
    "                break\n",
    "\n",
    "        with open(jsonFile, 'w') as file:\n",
    "            # la sortie de Json est une variable ici c'est a\n",
    "            json.dump(articles, file, indent=2, ensure_ascii=False)\n",
    "    else:\n",
    "        with open(jsonFile) as file:\n",
    "            articles = json.load(file)\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractInfoArticle(id, online=False):\n",
    "\n",
    "    article = {}\n",
    "    url = 'https://pagellapolitica.it/'\n",
    "    if online:\n",
    "        uriArticle = 'dichiarazioni/'\n",
    "        address = url + uriArticle + id\n",
    "        print(address)\n",
    "        resp = requests.get(address)\n",
    "        if resp:\n",
    "            source = requests.get(address).text\n",
    "            soup = bs.BeautifulSoup(source, \"lxml\")\n",
    "\n",
    "            # auteur\n",
    "            p = soup.find(\n",
    "                'p', {'class': 'h4 mb-1 px-2 text-dark font-weight-light'})\n",
    "            author = p.find('a', {'class': 'u-link-muted'}).text.strip()\n",
    "\n",
    "            # le titre au complet quand on rentre dans l'article\n",
    "            divFullBody = soup.find('div', {'class': 'col-lg-9 mb-9 mb-lg-0'})\n",
    "            if divFullBody:\n",
    "                # veracity\n",
    "                # h2 mark-ni\n",
    "                # h2 mark-vero\n",
    "                # h2 mark-ceri\n",
    "                # h2 mark-pinocchio\n",
    "                # h2 mark-panzana\n",
    "\n",
    "                veracities = [\"mark-ni\", \"mark-vero\",\n",
    "                              \"mark-ceri\", \"mark-pinocchio\", \"mark-panzana\"]\n",
    "                divTitle = divFullBody.find('div', {'class': 'mb-2 mt-2 px-2'})\n",
    "                divTitlePH2Px2 = divFullBody.find(\n",
    "                    'p', {'class': 'h2 px-2'})\n",
    "                if divTitle:\n",
    "                    fullTitle = divTitle.find(\n",
    "                        'span', {'class': 'h2'}).text.strip()\n",
    "\n",
    "                    spanVeracity = divTitle.find(\n",
    "                        'span', {'class': 'h2'})\n",
    "                    thisVeracity = spanVeracity['class']\n",
    "                    veracity = common(thisVeracity, veracities)[0]\n",
    "                elif divTitlePH2Px2:\n",
    "                    fullTitle = divTitlePH2Px2.find(\n",
    "                        'span', {'class': 'text-darker'}).text.strip()\n",
    "                    spanVeracity = divTitlePH2Px2.find(\n",
    "                        'span', {'class': 'text-darker'})\n",
    "                    thisVeracity = spanVeracity['class']\n",
    "                    veracity = common(thisVeracity, veracities)[0]\n",
    "\n",
    "                else:\n",
    "                    # dans les anciens artictles le fullText n'existe pas donc\n",
    "                    # si il ne le trouve pas il affiche le msg\n",
    "                    fullTitle = \"No fullTitle\"\n",
    "                    veracity = \"No veracity\"\n",
    "\n",
    "            # les claims\n",
    "            pclaimPX3 = soup.find(\n",
    "                'p', {'class': 'lead px-3 py-3 bg-light g-brd-around g-brd-lightblue'})\n",
    "            pclaimPX2 = soup.find('p', {'class': 'h2 px-2'})\n",
    "            if pclaimPX3:\n",
    "                claim = pclaimPX3.text.strip()\n",
    "            elif pclaimPX2:\n",
    "                claim = pclaimPX2.find(\n",
    "                    'span', {'class': 'text-darker'}).text.strip()\n",
    "\n",
    "            # les dates(publication,origine) elles sont dans la méme classe d'ou l'utilisation\n",
    "            # de divDates[0] pour la publication et divDates[1] pour l'origine\n",
    "            divDate = soup.find('div', {'class': 'card-body pt-0 px-2'})\n",
    "            divDates = divDate.findAll('div', {'class': 'col-lg-2 text-left'})\n",
    "            datePublished = divDates[0].find(\n",
    "                'span', {'class': 'text-dark'}).text.strip()\n",
    "            dateOrigin = divDates[1].find(\n",
    "                'span', {'class': 'text-dark'}).text.strip()\n",
    "\n",
    "            # statement source\n",
    "            divReferredLinks = divDate.find('div', {'class': 'col-lg-4'})\n",
    "            statementSource = divReferredLinks.find(\n",
    "                'a', {'class': 'u-link-muted'})['href']\n",
    "\n",
    "            # le join est pour concaténé les textes des balises P pour les mettre en un seul txt\n",
    "            divMainArticle = soup.find('div', {'id': 'analisi-content'})\n",
    "            listOfPInMainArticle = divMainArticle.findAll('p')\n",
    "            mainText = [text.text.strip() for text in listOfPInMainArticle]\n",
    "            mainArticle = ' '.join(mainText)\n",
    "\n",
    "            # les liens de reference dans le main text\n",
    "            # listOfAllLinks = divMainArticle.findAll('a')\n",
    "            for link in divMainArticle.findAll('a', href=True):\n",
    "                print(link)\n",
    "                if link['href']:\n",
    "                    print(\"link:\",link['href'])\n",
    "            #listOfAllLinksHref = [link['href'] for link in divMainArticle.findAll('a') if link['href']]\n",
    "\n",
    "            # Tags\n",
    "            if divFullBody.find('div', {'class': 'px-2 u-space-2-top'}):\n",
    "                divTags = divFullBody.find(\n",
    "                    'div', {'class': 'px-2 u-space-2-top'})\n",
    "                tags = divTags.findAll(\n",
    "                    'a', {'class': 'btn btn-sm btn-light u-btn-light transition-3d-hover rounded-0 mb-2'})\n",
    "\n",
    "                tagList = []\n",
    "                for tag in tags:\n",
    "                    tagList.append(tag.text.strip())\n",
    "\n",
    "    else:\n",
    "        data = readLinksJson()\n",
    "        # pour l'ajout d'un element dans un dictionnaire exsitant ,**dic[2] recupére\n",
    "        # tout les anciens elements du dic\n",
    "        data[id] = {\n",
    "            **data[id],\n",
    "            # le site de fact checking\n",
    "            \"source\": \"source not found\",\n",
    "            \"claim\": \"claim not found\",\n",
    "            \"body\": \"mainArticle not found\",                     # le text de l'article\n",
    "                    \"referred_links\": \"listOfAllLinksHref not found\",    # tous les liens dans le texte\n",
    "                    \"title\": \"fullTitle not found\",                      # le titre de l'article\n",
    "                    \"date\": \"dateOrigin not found\",                      # date de la claim\n",
    "                    \"url\": \"address not found\",  # url de l'article\n",
    "                    \"tags\": \"tags not found\",                          # les mots cles\n",
    "                    \"author\": \"author not found\",                       # auteur de la claim\n",
    "                    \"datePublished\": \"datePublished not found\",\n",
    "                    \"rating_value\": \"rating_value not found\",          # la valeur de la veracite\n",
    "                    \"statementSource\": \"statementSource not found\",\n",
    "                    # les entities nomes qui est extraite de la claim\n",
    "                    \"claim_entities\": \"claim_entities not found\",\n",
    "                    # les entities nomes qui est extraite de l'article\n",
    "                    \"body_entities\": \"body_entities not found\",\n",
    "                    # parmi les tages, les entities nomes a partir de la tag\n",
    "                    \"keyword_entities\": \"keyword_entities not found\",\n",
    "                    # les entities nomes a partir de l'auteur de la claim\n",
    "                    \"author_entities\": \"author_entities not found\",\n",
    "                    \"review_author\": \"review_author not found\"         # l'auteur de l'article\n",
    "\n",
    "        }\n",
    "\n",
    "    article = {\n",
    "        id: {\n",
    "            \"source\": url,                           # le site de fact checking\n",
    "            \"claim\": claim,\n",
    "            \"body\": mainArticle,                     # le text de l'article\n",
    "            \"referred_links\": listOfAllLinksHref,    # tous les liens dans le texte\n",
    "            \"title\": fullTitle,                      # le titre de l'article\n",
    "            \"date\": dateOrigin,                      # date de la claim\n",
    "            \"url\": address,  # url de l'article\n",
    "            \"tags\": tagList,                          # les mots cles\n",
    "            \"author\": author,                       # auteur de la claim\n",
    "            \"datePublished\": datePublished,\n",
    "            \"rating_value\": veracity,          # la valeur de la veracite\n",
    "            \"statementSource\": statementSource,\n",
    "            # les entities nomes qui est extraite de la claim\n",
    "            \"claim_entities\": \"claim_entities not found\",\n",
    "            # les entities nomes qui est extraite de l'article\n",
    "            \"body_entities\": \"body_entities not found\",\n",
    "            # parmi les tages, les entities nomes a partir de la tag\n",
    "            \"keyword_entities\": \"keyword_entities not found\",\n",
    "            # les entities nomes a partir de l'auteur de la claim\n",
    "            \"author_entities\": \"author_entities not found\",\n",
    "            \"review_author\": \"review_author not found\"         # l'auteur de l'article\n",
    "\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://pagellapolitica.it/dichiarazioni/1327\n",
      "<a href=\"http://www.quirinale.it/qrnw/statico/costituzione/costituzione.htm#Principi\">l’articolo 117 comma 4 della Costituzione</a>\n",
      "link: http://www.quirinale.it/qrnw/statico/costituzione/costituzione.htm#Principi\n",
      "<a href=\"http://www.salute.gov.it/ministero/sezMinistero.jsp?label=ssn\">la competenza concorrente  in ambito sanitario</a>\n",
      "link: http://www.salute.gov.it/ministero/sezMinistero.jsp?label=ssn\n",
      "<a href=\"http://www.regione.piemonte.it/pianosanitario/dwd/piano_socio_sanitario.pdf\">La riforma sanitaria, per la Regione Piemonte, prevede tagli ulteriori alla spesa sanitaria, diminuendo il numero di ospedali e servizi al cittadino</a>\n",
      "link: http://www.regione.piemonte.it/pianosanitario/dwd/piano_socio_sanitario.pdf\n",
      "<a href=\"http://www.salute.gov.it/portale/news/p3_2_1_1_1.jsp?lingua=italiano&amp;menu=notizie&amp;p=dalministero&amp;id=936\">Come visibile sul sito del Ministero della Salute, il Piemonte è l’unica Regione settentrionale impegnata nei piani di rientro finanziario</a>\n",
      "link: http://www.salute.gov.it/portale/news/p3_2_1_1_1.jsp?lingua=italiano&menu=notizie&p=dalministero&id=936\n",
      "<a href=\"http://www.parlamento.it/parlam/leggi/deleghe/98112dl.htm\">decreto legislativo 112 del 1998</a>\n",
      "link: http://www.parlamento.it/parlam/leggi/deleghe/98112dl.htm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'listOfAllLinksHref' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-ad0149e9fa94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mextractInfoArticle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1327\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-74-dff7dd0bbb9b>\u001b[0m in \u001b[0;36mextractInfoArticle\u001b[0;34m(id, online)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;34m\"claim\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mclaim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;34m\"body\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmainArticle\u001b[0m\u001b[0;34m,\u001b[0m                     \u001b[0;31m# le text de l'article\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0;34m\"referred_links\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlistOfAllLinksHref\u001b[0m\u001b[0;34m,\u001b[0m    \u001b[0;31m# tous les liens dans le texte\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m             \u001b[0;34m\"title\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfullTitle\u001b[0m\u001b[0;34m,\u001b[0m                      \u001b[0;31m# le titre de l'article\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;34m\"date\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdateOrigin\u001b[0m\u001b[0;34m,\u001b[0m                      \u001b[0;31m# date de la claim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'listOfAllLinksHref' is not defined"
     ]
    }
   ],
   "source": [
    "extractInfoArticle(\"1327\", online=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pageLinks(pageNumber):\n",
    "    url = 'https://pagellapolitica.it/'\n",
    "    uri = 'dichiarazioni/verificato?page='\n",
    "    pageAddress = url + uri + pageNumber\n",
    "    resp = requests.get(pageAddress)\n",
    "    if resp:\n",
    "        source = requests.get(pageAddress).text\n",
    "        soup = bs.BeautifulSoup(source, \"lxml\")\n",
    "        tables = soup.findAll('div', {'class': 'col-lg-3 mb-7'})\n",
    "        pageLinks = []\n",
    "        for table in tables:\n",
    "            link = table.find('a', {'class': 'statement-link'})['href']\n",
    "            articleLink = url+link\n",
    "            pageLinks.append(articleLink)\n",
    "    return pageLinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://pagellapolitica.it/dichiarazioni/8548/no-nel-2008-la-germania-non-ha-dato-300-miliardi-alle-sue-banche\n",
      "https://pagellapolitica.it/dichiarazioni/8547/emiliano-impreciso-sui-pugliesi-militari-e-su-quelli-fuggiti-dal-nord\n",
      "https://pagellapolitica.it/dichiarazioni/8546/renzi-esagera-sui-detenuti-nelle-carceri-italiane-in-un-verso-e-nellaltro\n",
      "https://pagellapolitica.it/dichiarazioni/8545/salvini-esagera-un-po-sul-pil-di-lombardia-e-veneto\n",
      "https://pagellapolitica.it/dichiarazioni/8544/che-cosa-dicono-i-primi-dati-sui-morti-con-il-coronavirus-in-italia\n",
      "https://pagellapolitica.it/dichiarazioni/8543/la-germania-e-meno-vecchia-anche-grazie-ai-migranti\n",
      "https://pagellapolitica.it/dichiarazioni/8542/la-sanita-italiana-e-davvero-uneccellenza-mondiale\n",
      "https://pagellapolitica.it/dichiarazioni/8541/di-maio-e-la-geografia-della-quarantena\n",
      "https://pagellapolitica.it/dichiarazioni/8540/e-vero-le-mascherine-sono-inutili-per-chi-e-sano\n",
      "https://pagellapolitica.it/dichiarazioni/8539/laumento-delliva-in-giappone-ha-danneggiato-leconomia\n",
      "https://pagellapolitica.it/dichiarazioni/8538/la-covid-19-e-davvero-poco-piu-di-una-normale-influenza\n",
      "https://pagellapolitica.it/dichiarazioni/8537/il-debito-pubblico-e-aumentato-per-colpa-del-reddito-di-cittadinanza\n",
      "https://pagellapolitica.it/dichiarazioni/8536/meno-nascite-causeranno-un-crollo-del-pil\n",
      "https://pagellapolitica.it/dichiarazioni/8535/come-leggere-i-numeri-sui-morti-per-linfluenza-senza-sbagliare\n",
      "https://pagellapolitica.it/dichiarazioni/8534/davvero-la-ue-non-esiste-nel-contrasto-al-nuovo-coronavirus\n",
      "https://pagellapolitica.it/dichiarazioni/8533/no-ue-e-istat-non-hanno-certificato-risultati-incredibili-per-il-reddito-di-cittadinanza\n"
     ]
    }
   ],
   "source": [
    "pageLinks = pageLinks(\"1\")\n",
    "for i in pageLinks:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the welcome page we see only 9 articles but when we see the HTML code, there are 16 article's link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "print(len(pageLinks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idNumber(link):\n",
    "    idPattern = re.search(r'/([0-9]+)/', link)\n",
    "    idNumber = idPattern.group(1)\n",
    "    return idNumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://pagellapolitica.it/dichiarazioni/8548\n",
      "rating_value =  mark-ni\n",
      "\n",
      "\n",
      "https://pagellapolitica.it/dichiarazioni/8547\n",
      "rating_value =  mark-ni\n",
      "\n",
      "\n",
      "https://pagellapolitica.it/dichiarazioni/8546\n",
      "rating_value =  mark-ni\n",
      "\n",
      "\n",
      "https://pagellapolitica.it/dichiarazioni/8545\n",
      "rating_value =  mark-ceri\n",
      "\n",
      "\n",
      "https://pagellapolitica.it/dichiarazioni/8544\n",
      "rating_value =  mark-vero\n",
      "\n",
      "\n",
      "https://pagellapolitica.it/dichiarazioni/8543\n",
      "rating_value =  mark-ceri\n",
      "\n",
      "\n",
      "https://pagellapolitica.it/dichiarazioni/8542\n",
      "rating_value =  mark-ceri\n",
      "\n",
      "\n",
      "https://pagellapolitica.it/dichiarazioni/8541\n",
      "rating_value =  mark-ceri\n",
      "\n",
      "\n",
      "https://pagellapolitica.it/dichiarazioni/8540\n",
      "rating_value =  mark-vero\n",
      "\n",
      "\n",
      "https://pagellapolitica.it/dichiarazioni/8539\n",
      "rating_value =  mark-ceri\n",
      "\n",
      "\n",
      "https://pagellapolitica.it/dichiarazioni/8538\n",
      "rating_value =  mark-ni\n",
      "\n",
      "\n",
      "https://pagellapolitica.it/dichiarazioni/8537\n",
      "rating_value =  mark-ni\n",
      "\n",
      "\n",
      "https://pagellapolitica.it/dichiarazioni/8536\n",
      "rating_value =  mark-ni\n",
      "\n",
      "\n",
      "https://pagellapolitica.it/dichiarazioni/8535\n",
      "rating_value =  mark-pinocchio\n",
      "\n",
      "\n",
      "https://pagellapolitica.it/dichiarazioni/8534\n",
      "rating_value =  mark-ni\n",
      "\n",
      "\n",
      "https://pagellapolitica.it/dichiarazioni/8533\n",
      "rating_value =  mark-pinocchio\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#for pageNumber in range(42, 174):\n",
    "pageURLs = pageLinks(\"1\")\n",
    "for page in pageURLs:\n",
    "    id = idNumber(page)\n",
    "    lastArticle = extractInfoArticle(id, online=True)\n",
    "    #ter.writeLinksJson(lastArticle, append=True)\n",
    "    print('rating_value = ', lastArticle[id][\"rating_value\"])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findPageNumberOfID(id):\n",
    "    for pageNumber in range(1, 200):\n",
    "        print(\"Page:\", pageNumber)\n",
    "        pageURLs = pageLinks(str(pageNumber))\n",
    "        for page in pageURLs:\n",
    "            idWanted = idNumber(page)\n",
    "            if idWanted == id:\n",
    "                print(\"ID\", id, \"is in the page:\", pageNumber)\n",
    "                break\n",
    "        if idWanted == id:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 1\n",
      "Page: 2\n",
      "Page: 3\n",
      "Page: 4\n",
      "Page: 5\n",
      "Page: 6\n",
      "Page: 7\n",
      "Page: 8\n",
      "Page: 9\n",
      "Page: 10\n",
      "Page: 11\n",
      "Page: 12\n",
      "Page: 13\n",
      "Page: 14\n",
      "Page: 15\n",
      "Page: 16\n",
      "Page: 17\n",
      "Page: 18\n",
      "Page: 19\n",
      "Page: 20\n",
      "Page: 21\n",
      "Page: 22\n",
      "Page: 23\n",
      "Page: 24\n",
      "Page: 25\n",
      "Page: 26\n",
      "Page: 27\n",
      "Page: 28\n",
      "Page: 29\n",
      "Page: 30\n",
      "Page: 31\n",
      "Page: 32\n",
      "Page: 33\n",
      "Page: 34\n",
      "Page: 35\n",
      "Page: 36\n",
      "Page: 37\n",
      "Page: 38\n",
      "Page: 39\n",
      "Page: 40\n",
      "Page: 41\n",
      "Page: 42\n",
      "Page: 43\n",
      "Page: 44\n",
      "Page: 45\n",
      "Page: 46\n",
      "Page: 47\n",
      "Page: 48\n",
      "Page: 49\n",
      "Page: 50\n",
      "Page: 51\n",
      "Page: 52\n",
      "Page: 53\n",
      "Page: 54\n",
      "Page: 55\n",
      "Page: 56\n",
      "Page: 57\n",
      "Page: 58\n",
      "Page: 59\n",
      "Page: 60\n",
      "Page: 61\n",
      "Page: 62\n",
      "Page: 63\n",
      "Page: 64\n",
      "Page: 65\n",
      "Page: 66\n",
      "Page: 67\n",
      "Page: 68\n",
      "Page: 69\n",
      "Page: 70\n",
      "Page: 71\n",
      "Page: 72\n",
      "Page: 73\n",
      "Page: 74\n",
      "Page: 75\n",
      "Page: 76\n",
      "Page: 77\n",
      "Page: 78\n",
      "Page: 79\n",
      "Page: 80\n",
      "Page: 81\n",
      "Page: 82\n",
      "Page: 83\n",
      "Page: 84\n",
      "Page: 85\n",
      "Page: 86\n",
      "Page: 87\n",
      "Page: 88\n",
      "Page: 89\n",
      "Page: 90\n",
      "Page: 91\n",
      "Page: 92\n",
      "Page: 93\n",
      "Page: 94\n",
      "Page: 95\n",
      "Page: 96\n",
      "Page: 97\n",
      "Page: 98\n",
      "Page: 99\n",
      "Page: 100\n",
      "Page: 101\n",
      "Page: 102\n",
      "Page: 103\n",
      "Page: 104\n",
      "Page: 105\n",
      "Page: 106\n",
      "Page: 107\n",
      "Page: 108\n",
      "Page: 109\n",
      "Page: 110\n",
      "Page: 111\n",
      "Page: 112\n",
      "Page: 113\n",
      "Page: 114\n",
      "Page: 115\n",
      "Page: 116\n",
      "Page: 117\n",
      "Page: 118\n",
      "Page: 119\n",
      "Page: 120\n",
      "Page: 121\n",
      "Page: 122\n",
      "Page: 123\n",
      "Page: 124\n",
      "Page: 125\n",
      "Page: 126\n",
      "Page: 127\n",
      "Page: 128\n",
      "Page: 129\n",
      "Page: 130\n",
      "ID 1269 is in the page: 130\n"
     ]
    }
   ],
   "source": [
    "findPageNumberOfID(\"1269\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "parameters = ['data-prix','data-codepostal','data-idagence','data-idannonce','data-nb_chambres','data-nb_pieces','data-surface','data-typebien']\n",
    "df_f = pd.DataFrame()\n",
    "for par in parameters:\n",
    "    l = []\n",
    "    for el in em_box:\n",
    "        j = el[par]\n",
    "        l.append(j)\n",
    "    l = pd.DataFrame(l, columns = [par])\n",
    "    df_f = pd.concat([df_f,l], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [data-prix, data-codepostal, data-idagence, data-idannonce, data-nb_chambres, data-nb_pieces, data-surface, data-typebien]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(df_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
